# ğŸ¤– Available ML Models Guide

## Overview

The ML Pipeline now supports **9 different algorithms** for both classification and regression tasks. The LLM intelligently selects the best models based on your dataset characteristics.

---

## ğŸ“Š Classification Models

### 1. **Random Forest** (`random_forest`)

```python
RandomForestClassifier(random_state=42)
```

**Best For:**

- âœ… Medium to large datasets (500+ samples)
- âœ… Non-linear relationships
- âœ… Feature importance analysis
- âœ… Handling missing values
- âœ… Robust to outliers

**Hyperparameters Tuned:**

- `n_estimators`: 100, 200, 300
- `max_depth`: 10, 20, 30, None
- `min_samples_split`: 2, 5, 10
- `min_samples_leaf`: 1, 2, 4

**When LLM Suggests:** General-purpose, balanced accuracy

---

### 2. **XGBoost** (`xgboost`)

```python
XGBClassifier(random_state=42, eval_metric='logloss')
```

**Best For:**

- âœ… Structured/tabular data
- âœ… Winning Kaggle competitions
- âœ… Imbalanced datasets
- âœ… High accuracy requirements
- âœ… Gradient boosting power

**Hyperparameters Tuned:**

- `n_estimators`: 100, 200
- `max_depth`: 3, 5, 7
- `learning_rate`: 0.01, 0.1, 0.3
- `subsample`: 0.8, 1.0

**When LLM Suggests:** Healthcare/finance domains, structured data

---

### 3. **Logistic Regression** (`logistic_regression`)

```python
LogisticRegression(random_state=42, max_iter=1000)
```

**Best For:**

- âœ… Binary classification
- âœ… Interpretability required
- âœ… Small to medium datasets
- âœ… Linear relationships
- âœ… Fast training/inference

**Hyperparameters Tuned:**

- `C`: 0.01, 0.1, 1, 10
- `penalty`: l2
- `solver`: lbfgs, liblinear

**When LLM Suggests:** Need interpretability, linear patterns detected

---

### 4. **Support Vector Machine** (`svm`)

```python
SVC(random_state=42)
```

**Best For:**

- âœ… Small to medium datasets (<10k samples)
- âœ… High-dimensional data
- âœ… Clear margin of separation
- âœ… Non-linear decision boundaries (RBF kernel)

**Hyperparameters Tuned:**

- `C`: 0.1, 1, 10
- `kernel`: rbf, linear
- `gamma`: scale, auto

**When LLM Suggests:** Small dataset, complex boundaries

---

### 5. **Decision Tree** (`decision_tree`) â­ NEW!

```python
DecisionTreeClassifier(random_state=42)
```

**Best For:**

- âœ… Interpretability (visual tree)
- âœ… Fast training
- âœ… Handling categorical features
- âœ… No data preprocessing needed
- âš ï¸ Prone to overfitting

**Hyperparameters Tuned:**

- `max_depth`: 5, 10, 20, None
- `min_samples_split`: 2, 5, 10
- `min_samples_leaf`: 1, 2, 4
- `criterion`: gini, entropy

**When LLM Suggests:** Need quick baseline, interpretability critical

---

### 6. **Neural Network** (`neural_network` or `mlp`) â­ NEW!

```python
MLPClassifier(random_state=42, max_iter=1000)
```

**Best For:**

- âœ… Large datasets (1000+ samples)
- âœ… Complex non-linear patterns
- âœ… Deep feature learning
- âš ï¸ Requires data scaling
- âš ï¸ Slower training

**Hyperparameters Tuned:**

- `hidden_layer_sizes`: (50,), (100,), (50, 50), (100, 50)
- `activation`: relu, tanh
- `alpha`: 0.0001, 0.001, 0.01
- `learning_rate`: constant, adaptive

**When LLM Suggests:** Large dataset, complex patterns, high accuracy needed

---

## ğŸ“ˆ Regression Models

### 1. **Random Forest** (`random_forest`)

```python
RandomForestRegressor(random_state=42)
```

**Best For:**

- âœ… Non-linear relationships
- âœ… Feature importance
- âœ… Robust predictions
- âœ… Handles outliers well

**Same benefits as classification version**

---

### 2. **XGBoost** (`xgboost`)

```python
XGBRegressor(random_state=42)
```

**Best For:**

- âœ… Structured data
- âœ… High RÂ² scores
- âœ… Gradient boosting
- âœ… Production deployments

**Industry standard for structured data regression**

---

### 3. **Ridge Regression** (`ridge`)

```python
Ridge(random_state=42)
```

**Best For:**

- âœ… Linear relationships
- âœ… Multicollinearity
- âœ… Regularization needed
- âœ… Fast and interpretable

**Hyperparameters Tuned:**

- `alpha`: 0.01, 0.1, 1.0, 10.0, 100.0

**When LLM Suggests:** Linear patterns, need regularization

---

### 4. **Linear Regression** (`linear_regression`)

```python
Ridge(random_state=42)  # Uses Ridge with tuning
```

**Best For:**

- âœ… Simple baseline
- âœ… Interpretability
- âœ… Fast training
- âœ… Linear relationships

**When LLM Suggests:** Starting point, baseline model

---

### 5. **Support Vector Regression** (`svm`)

```python
SVR()
```

**Best For:**

- âœ… Small datasets
- âœ… Non-linear patterns
- âœ… Kernel methods
- âš ï¸ Slower on large data

---

### 6. **Decision Tree** (`decision_tree`) â­ NEW!

```python
DecisionTreeRegressor(random_state=42)
```

**Best For:**

- âœ… Interpretable predictions
- âœ… Fast training
- âœ… No scaling needed
- âš ï¸ Can overfit

---

### 7. **Neural Network** (`neural_network` or `mlp`) â­ NEW!

```python
MLPRegressor(random_state=42, max_iter=1000)
```

**Best For:**

- âœ… Large datasets
- âœ… Complex patterns
- âœ… Deep learning
- âš ï¸ Needs scaling

---

## ğŸ§  LLM Model Selection Logic

### Dataset Size-Based:

```
Small (<500 rows):
  â†’ LLM suggests: logistic_regression, svm, decision_tree

Medium (500-5000 rows):
  â†’ LLM suggests: random_forest, xgboost, neural_network

Large (5000+ rows):
  â†’ LLM suggests: xgboost, neural_network, random_forest
```

### Domain-Based:

```
Healthcare:
  â†’ Interpretability matters: logistic_regression, decision_tree
  â†’ Accuracy critical: xgboost, random_forest

Finance:
  â†’ Regulatory compliance: logistic_regression, decision_tree
  â†’ Fraud detection: xgboost, neural_network, random_forest

General:
  â†’ Balanced approach: xgboost, random_forest, neural_network
```

### Task-Based:

```
Binary Classification:
  â†’ logistic_regression, xgboost, neural_network

Multi-class Classification:
  â†’ random_forest, xgboost, neural_network

Regression:
  â†’ xgboost, ridge, neural_network
```

---

## âš™ï¸ Configuration

### Enable All Models in `config.yaml`:

```yaml
agents:
  model_tuning:
    enabled: true
    algorithms: # Ignored if LLM enabled
      - random_forest
      - xgboost
      - decision_tree

llm:
  reasoning_enabled: true # Let LLM choose models
  model: "llama3.1:8b"
```

### Manual Model Selection:

```yaml
agents:
  model_tuning:
    algorithms:
      - neural_network # Deep learning
      - decision_tree # Interpretable
      - xgboost # High accuracy
```

---

## ğŸ“Š Performance Comparison

### Speed (Training Time):

```
Fastest:  decision_tree < logistic_regression < linear_regression
Medium:   random_forest < svm
Slowest:  xgboost < neural_network
```

### Accuracy (General):

```
Highest:  xgboost â‰ˆ neural_network â‰ˆ random_forest
Medium:   svm â‰ˆ ridge
Lower:    decision_tree â‰ˆ logistic_regression
```

### Interpretability:

```
Most:     decision_tree > logistic_regression > linear_regression
Medium:   random_forest > ridge
Least:    neural_network < xgboost < svm
```

---

## ğŸ¯ Model Selection Flowchart

```
Start
â”‚
â”œâ”€ Need Interpretability?
â”‚  â”œâ”€ Yes â†’ decision_tree, logistic_regression, ridge
â”‚  â””â”€ No  â†’ Continue
â”‚
â”œâ”€ Large Dataset (>5000)?
â”‚  â”œâ”€ Yes â†’ xgboost, neural_network, random_forest
â”‚  â””â”€ No  â†’ Continue
â”‚
â”œâ”€ Complex Patterns?
â”‚  â”œâ”€ Yes â†’ xgboost, neural_network, random_forest
â”‚  â””â”€ No  â†’ logistic_regression, ridge, svm
â”‚
â””â”€ Time Constrained?
   â”œâ”€ Yes â†’ decision_tree, logistic_regression
   â””â”€ No  â†’ xgboost, neural_network
```

---

## ğŸš« About Clustering

**Note:** Clustering algorithms (K-Means, DBSCAN, etc.) are **unsupervised learning** and don't fit in this **supervised learning** pipeline since we have labeled data (target column).

If you need clustering:

1. Use for **exploratory data analysis** (separate script)
2. Create **features from clusters** (cluster ID as a feature)
3. Use in **anomaly detection** (outlier analysis)

But they cannot be used as primary models in a supervised classification/regression pipeline.

---

## ğŸ§ª Example LLM Suggestions

### Wine Quality (Regression, 1599 rows):

```
ğŸ§  LLM: "xgboost, ridge, random_forest"
Reasoning: Medium dataset, structured data, regression task
```

### Heart Disease (Classification, 303 rows):

```
ğŸ§  LLM: "logistic_regression, decision_tree, svm"
Reasoning: Small dataset, healthcare (interpretability), binary classification
```

### Fraud Detection (Classification, 50k rows):

```
ğŸ§  LLM: "xgboost, neural_network, random_forest"
Reasoning: Large dataset, imbalanced, needs high accuracy
```

---

## ğŸ“ Model Usage Tips

### 1. **Start Simple**

- Use decision_tree or logistic_regression as baseline
- Establish performance benchmark
- Understand data patterns

### 2. **Scale Up**

- Try xgboost or random_forest for better accuracy
- Use neural_network for complex patterns
- Compare performance gains

### 3. **Optimize**

- Hyperparameter tuning (done automatically)
- Feature engineering
- Ensemble methods

### 4. **Deploy**

- Choose based on accuracy + interpretability + speed
- Document model selection reasoning
- Monitor in production

---

## ğŸ“š When to Use Each Model

| Model          | Small Data | Large Data | Interpretable | Fast Training | High Accuracy |
| -------------- | ---------- | ---------- | ------------- | ------------- | ------------- |
| Decision Tree  | âœ…         | âŒ         | âœ…            | âœ…            | âš ï¸            |
| Logistic Reg   | âœ…         | âš ï¸         | âœ…            | âœ…            | âš ï¸            |
| SVM            | âœ…         | âŒ         | âŒ            | âš ï¸            | âœ…            |
| Random Forest  | âš ï¸         | âœ…         | âš ï¸            | âš ï¸            | âœ…            |
| XGBoost        | âœ…         | âœ…         | âŒ            | âŒ            | âœ…            |
| Neural Network | âŒ         | âœ…         | âŒ            | âŒ            | âœ…            |
| Ridge          | âœ…         | âœ…         | âœ…            | âœ…            | âš ï¸            |

âœ… = Excellent, âš ï¸ = Good, âŒ = Not Recommended

---

## ğŸš€ Getting Started

### Run with LLM Model Selection:

```bash
# LLM chooses best models automatically
streamlit run app.py
```

### Run with Specific Models:

```bash
# Edit config.yaml first
llm:
  reasoning_enabled: false

agents:
  model_tuning:
    algorithms:
      - neural_network
      - xgboost
      - decision_tree
```

---

**The LLM will intelligently select the best models based on your data!** ğŸ§ âœ¨

