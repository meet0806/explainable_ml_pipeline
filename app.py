"""
Streamlit Web UI for Explainable ML Agentic Pipeline
Healthcare and Finance Domain Application
"""

import streamlit as st
import pandas as pd
import yaml
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import os
import sys

# Add src to path
sys.path.append('.')

from src.orchestrator import Orchestrator
from src.utils.run_history import RunHistory
from src.utils.explainability_viz import (
    plot_shap_feature_importance,
    plot_lime_feature_importance,
    plot_lime_instance_explanation,
    create_explainability_summary_table,
    compare_explainability_methods
)

# Page config
st.set_page_config(
    page_title="Explainable ML Pipeline",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1E88E5;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    .success-box {
        background-color: #d4edda;
        border-left: 5px solid #28a745;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border-left: 5px solid #ffc107;
        padding: 1rem;
        margin: 1rem 0;
    }
    .agent-card {
        background: white;
        border: 2px solid #e0e0e0;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)


def load_config():
    """Load configuration"""
    try:
        with open('config.yaml', 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        st.error(f"Error loading config: {e}")
        return None


def save_uploaded_file(uploaded_file):
    """Save uploaded file temporarily"""
    try:
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return file_path
    except Exception as e:
        st.error(f"Error saving file: {e}")
        return None


def display_dataframe_info(df):
    """Display dataset information"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üìä Rows", f"{len(df):,}")
    with col2:
        st.metric("üìã Columns", len(df.columns))
    with col3:
        st.metric("üíæ Memory", f"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    with col4:
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100)
        st.metric("‚ö†Ô∏è Missing %", f"{missing_pct:.1f}%")


def plot_metrics(metrics, task_type):
    """Plot performance metrics"""
    if task_type == "classification":
        metric_names = ['accuracy', 'precision', 'recall', 'f1_score']
        metric_values = [metrics.get(m, 0) for m in metric_names]
        
        fig = go.Figure(data=[
            go.Bar(
                x=metric_names,
                y=metric_values,
                marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'],
                text=[f"{v:.3f}" for v in metric_values],
                textposition='auto',
            )
        ])
        
        fig.update_layout(
            title="Classification Metrics",
            yaxis_title="Score",
            yaxis_range=[0, 1],
            height=400
        )
        
    else:  # regression
        metric_names = ['r2_score', 'rmse', 'mae']
        metric_values = [metrics.get(m, 0) for m in metric_names]
        
        fig = go.Figure(data=[
            go.Bar(
                x=metric_names,
                y=metric_values,
                marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'],
                text=[f"{v:.3f}" for v in metric_values],
                textposition='auto',
            )
        ])
        
        fig.update_layout(
            title="Regression Metrics",
            yaxis_title="Score",
            height=400
        )
    
    return fig


def display_confusion_matrix(metrics):
    """Display confusion matrix"""
    if 'confusion_matrix' in metrics:
        cm = metrics['confusion_matrix']
        
        fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=['Predicted 0', 'Predicted 1'],
            y=['Actual 0', 'Actual 1'],
            colorscale='Blues',
            text=cm,
            texttemplate='%{text}',
            textfont={"size": 20},
        ))
        
        fig.update_layout(
            title="Confusion Matrix",
            height=400
        )
        
        return fig
    return None


def main():
    """Main Streamlit app"""
    
    # Initialize run history
    if 'run_history' not in st.session_state:
        st.session_state['run_history'] = RunHistory()
    
    run_history = st.session_state['run_history']
    
    # Header
    st.markdown('<div class="main-header">ü§ñ Explainable ML Pipeline</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Agentic AI for Healthcare and Finance</div>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.image("https://img.icons8.com/clouds/200/000000/artificial-intelligence.png", width=150)
        st.title("‚öôÔ∏è Configuration")
        
        # Load config
        config = load_config()
        
        if config:
            # Task type
            task_type = st.selectbox(
                "üìã Task Type",
                ["classification", "regression"],
                index=0
            )
            
            # Domain
            domain = st.selectbox(
                "üè• Domain",
                ["healthcare", "finance", "general"],
                index=0
            )
            
            # LLM Settings
            st.subheader("üß† LLM Settings")
            llm_enabled = st.checkbox(
                "Enable LLM Reasoning",
                value=config.get("llm", {}).get("reasoning_enabled", False)
            )
            
            if llm_enabled:
                llm_model = st.selectbox(
                    "Model",
                    ["llama3.1:8b", "llama3.1:70b", "mistral:7b"],
                    index=0
                )
            
            # Agent Settings
            st.subheader("ü§ñ Agent Settings")
            
            with st.expander("Model Algorithms"):
                algorithms = st.multiselect(
                    "Select Algorithms",
                    ["random_forest", "xgboost", "logistic_regression", "svm"],
                    default=["random_forest", "xgboost"]
                )
            
            with st.expander("Advanced Settings"):
                cv_folds = st.slider("CV Folds", 3, 10, 5)
                perf_threshold = st.slider("Performance Threshold", 0.5, 1.0, 0.75, 0.05)
                max_retrain = st.slider("Max Retrain Cycles", 1, 5, 3)
            
            st.divider()
            
            # Run History Section
            st.subheader("üìú Run History")
            
            all_runs = run_history.get_all_runs()
            
            if all_runs:
                stats = run_history.get_stats()
                
                # Quick stats
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Total Runs", stats['total_runs'])
                with col2:
                    st.metric("Deployed", stats['successful_deployments'])
                
                # Run selector
                run_options = {
                    f"{run['run_name']} ({run['task_type']})": run['run_id'] 
                    for run in all_runs
                }
                
                selected_run_name = st.selectbox(
                    "Load Previous Run",
                    options=list(run_options.keys()),
                    index=None,
                    placeholder="Select a run to load..."
                )
                
                if selected_run_name:
                    selected_run_id = run_options[selected_run_name]
                    
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        if st.button("üìÇ Load Run", use_container_width=True):
                            # Load the selected run
                            loaded_results = run_history.get_run(selected_run_id)
                            if loaded_results:
                                st.session_state['results'] = loaded_results
                                st.session_state['task_type'] = run_history.get_run_metadata(selected_run_id)['task_type']
                                st.session_state['current_run_id'] = selected_run_id
                                st.success(f"‚úÖ Loaded: {selected_run_name}")
                                st.rerun()
                    
                    with col2:
                        if st.button("üóëÔ∏è", use_container_width=True):
                            if run_history.delete_run(selected_run_id):
                                st.success("Deleted!")
                                st.rerun()
            else:
                st.info("No runs yet. Start your first pipeline!")
            
            st.divider()
            st.caption("v1.0.0 | Built with ‚ù§Ô∏è")
    
    # Main content
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üì§ Upload & Run", "üìä Results", "ü§ñ Agents", "üìà Visualizations", "üîÑ Compare Runs"])
    
    with tab1:
        st.header("üì§ Upload Dataset")
        
        # File upload
        uploaded_file = st.file_uploader(
            "Choose a CSV file",
            type=['csv'],
            help="Upload your dataset in CSV format"
        )
        
        if uploaded_file is not None:
            # Load data
            df = pd.read_csv(uploaded_file)
            
            st.success(f"‚úÖ File uploaded successfully: {uploaded_file.name}")
            
            # Display data info
            st.subheader("üìä Dataset Preview")
            display_dataframe_info(df)
            
            # Show first rows
            st.dataframe(df.head(10), use_container_width=True)
            
            # Target column selection
            st.subheader("üéØ Select Target Column")
            
            col1, col2 = st.columns([3, 1])
            with col1:
                target_column = st.selectbox(
                    "Target Variable",
                    options=df.columns.tolist(),
                    index=len(df.columns)-1
                )
            
            with col2:
                st.metric("Unique Values", df[target_column].nunique())
            
            # Target distribution
            st.subheader("üìà Target Distribution")
            target_counts = df[target_column].value_counts()
            
            fig = px.bar(
                x=target_counts.index,
                y=target_counts.values,
                labels={'x': target_column, 'y': 'Count'},
                title=f"Distribution of {target_column}"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Run pipeline button
            st.divider()
            
            run_col1, run_col2, run_col3 = st.columns([1, 2, 1])
            with run_col2:
                run_button = st.button(
                    "üöÄ Run ML Pipeline",
                    type="primary",
                    use_container_width=True
                )
            
            if run_button:
                # Save file
                file_path = save_uploaded_file(uploaded_file)
                
                # Update config
                if llm_enabled:
                    config['llm']['reasoning_enabled'] = True
                    config['llm']['model'] = llm_model
                
                config['agents']['model_tuning']['algorithms'] = algorithms
                config['agents']['model_tuning']['cv_folds'] = cv_folds
                config['agents']['judge']['min_performance_threshold'] = perf_threshold
                config['agents']['judge']['max_retrain_cycles'] = max_retrain
                
                # Run pipeline
                try:
                    # Initialize orchestrator
                    orchestrator = Orchestrator(config)
                    
                    # Create progress placeholders
                    progress_container = st.container()
                    with progress_container:
                        st.markdown("### üîÑ Pipeline Execution Progress")
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        agent_status = st.empty()
                        time_elapsed = st.empty()
                    
                    import time
                    start_time = time.time()
                    
                    # Progress tracking state
                    agent_states = {
                        "eda": "‚è∏Ô∏è Waiting",
                        "feature_engineering": "‚è∏Ô∏è Waiting",
                        "model_tuning": "‚è∏Ô∏è Waiting",
                        "evaluator": "‚è∏Ô∏è Waiting",
                        "judge": "‚è∏Ô∏è Waiting"
                    }
                    
                    # Progress callback function
                    def update_progress(agent_name, iteration):
                        # Map internal names to display names
                        display_names = {
                            "eda": "EDA Agent",
                            "feature_engineering": "Feature Engineering",
                            "model_tuning": "Model Tuning",
                            "evaluator": "Evaluator",
                            "judge": "Judge"
                        }
                        
                        # Update states
                        for key in agent_states.keys():
                            if key == agent_name:
                                agent_states[key] = "‚è≥ Running..."
                            elif list(agent_states.keys()).index(key) < list(agent_states.keys()).index(agent_name):
                                agent_states[key] = "‚úÖ Complete"
                        
                        # Calculate progress
                        progress_map = {"eda": 20, "feature_engineering": 40, "model_tuning": 60, "evaluator": 80, "judge": 90}
                        progress = progress_map.get(agent_name, 10)
                        
                        # Update UI
                        progress_bar.progress(progress)
                        status_text.markdown(f"**Status:** Running {display_names.get(agent_name, agent_name)} (Iteration {iteration})...")
                        
                        # Update agent status
                        status_md = "**Agent Pipeline:**\n"
                        for key, display_name in display_names.items():
                            status_md += f"- {agent_states[key]} {display_name}\n"
                        agent_status.markdown(status_md)
                        
                        # Update time
                        elapsed = time.time() - start_time
                        time_elapsed.markdown(f"‚è±Ô∏è **Elapsed Time:** {elapsed/60:.1f} minutes")
                    
                    # Initialize display
                    status_text.markdown("**Status:** Initializing agents...")
                    progress_bar.progress(10)
                    agent_status.markdown("""
                    **Agent Pipeline:**
                    - ‚è∏Ô∏è Waiting EDA Agent
                    - ‚è∏Ô∏è Waiting Feature Engineering
                    - ‚è∏Ô∏è Waiting Model Tuning
                    - ‚è∏Ô∏è Waiting Evaluator
                    - ‚è∏Ô∏è Waiting Judge
                    """)
                    
                    # Run pipeline with progress callback
                    results = orchestrator.run_pipeline(
                        data=df,
                        target_column=target_column,
                        task_type=task_type,
                        domain=domain,
                        progress_callback=update_progress
                    )
                    
                    # Mark all complete
                    for key in agent_states.keys():
                        agent_states[key] = "‚úÖ Complete"
                    agent_status.markdown("""
                    **Agent Pipeline:**
                    - ‚úÖ Complete EDA Agent
                    - ‚úÖ Complete Feature Engineering
                    - ‚úÖ Complete Model Tuning
                    - ‚úÖ Complete Evaluator
                    - ‚úÖ Complete Judge
                    """)
                    
                    progress_bar.progress(100)
                    status_text.text("‚úÖ Pipeline completed!")
                    
                    # Store results in session state
                    st.session_state['results'] = results
                    st.session_state['task_type'] = task_type
                    st.session_state['orchestrator'] = orchestrator
                    
                    # Save run to history
                    dataset_name = uploaded_file.name if uploaded_file else "demo_dataset"
                    run_id = run_history.save_run(
                        results=results,
                        dataset_name=dataset_name,
                        task_type=task_type,
                        domain=domain,
                        target_column=target_column,
                        dataset_shape=df.shape
                    )
                    st.session_state['current_run_id'] = run_id
                    
                    st.success("üéâ Pipeline completed successfully!")
                    st.balloons()
                    
                    # Display quick summary
                    final_results = results['final_results']
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric(
                            "Model Approved",
                            "‚úÖ Yes" if final_results['model_approved'] else "‚ùå No"
                        )
                    with col2:
                        st.metric(
                            "Best Model",
                            final_results['best_model']
                        )
                    with col3:
                        st.metric(
                            "Performance",
                            f"{final_results['performance_score']:.3f}"
                        )
                    with col4:
                        st.metric(
                            "Iterations",
                            results['pipeline_info']['total_iterations']
                        )
                    
                except Exception as e:
                    st.error(f"‚ùå Error running pipeline: {str(e)}")
                    st.exception(e)
    
    with tab2:
        st.header("üìä Pipeline Results")
        
        if 'results' in st.session_state:
            results = st.session_state['results']
            final_results = results['final_results']
            
            # Status cards
            col1, col2 = st.columns(2)
            
            with col1:
                if final_results['model_approved']:
                    st.markdown(
                        '<div class="success-box"><h3>‚úÖ Model Approved</h3><p>The model meets performance thresholds.</p></div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        '<div class="warning-box"><h3>‚ö†Ô∏è Model Not Approved</h3><p>Performance below threshold. Retraining recommended.</p></div>',
                        unsafe_allow_html=True
                    )
            
            with col2:
                if final_results['deployment_ready']:
                    st.markdown(
                        '<div class="success-box"><h3>üöÄ Deployment Ready</h3><p>Model is ready for production deployment.</p></div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        '<div class="warning-box"><h3>‚è∏Ô∏è Not Ready</h3><p>Additional validation required.</p></div>',
                        unsafe_allow_html=True
                    )
            
            # All Models Comparison
            st.subheader("üî¨ All Models Trained & Compared")
            
            # Extract all models from iterations
            all_models_data = []
            for iter_key, iter_data in results.get('all_iterations', {}).items():
                if 'model_tuning' in iter_data:
                    model_tuning = iter_data['model_tuning']
                    for model in model_tuning.get('all_models', []):
                        all_models_data.append({
                            'Model': model['algorithm'].replace('_', ' ').title(),
                            'CV Score': f"{model['cv_score']:.4f}",
                            'CV Score (numeric)': model['cv_score'],
                            'Parameters': ', '.join([f"{k}={v}" for k, v in model.get('best_params', {}).items()][:3]),
                            'Selected': '‚úÖ Best' if model['algorithm'] == final_results['best_model'] else ''
                        })
            
            if all_models_data:
                # Summary metrics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Models Trained", len(all_models_data))
                with col2:
                    best_model_name = final_results['best_model'].replace('_', ' ').title()
                    st.metric("Best Model", best_model_name)
                with col3:
                    best_cv_score = max([m['CV Score (numeric)'] for m in all_models_data])
                    st.metric("Best CV Score", f"{best_cv_score:.4f}")
                
                # Create DataFrame
                models_df = pd.DataFrame(all_models_data)
                models_df = models_df.drop('CV Score (numeric)', axis=1)  # Remove numeric column used for calculations
                
                # Display as styled table
                st.dataframe(
                    models_df,
                    use_container_width=True,
                    hide_index=True
                )
                
                # Visual comparison chart
                if len(all_models_data) > 1:
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        fig = go.Figure()
                        
                        colors = ['#2ecc71' if m['Selected'] == '‚úÖ Best' else '#3498db' for m in all_models_data]
                        
                        fig.add_trace(go.Bar(
                            x=[m['Model'] for m in all_models_data],
                            y=[float(m['CV Score']) for m in all_models_data],
                            marker_color=colors,
                            text=[m['CV Score'] for m in all_models_data],
                            textposition='auto',
                            hovertemplate='<b>%{x}</b><br>CV Score: %{y:.4f}<extra></extra>'
                        ))
                        
                        fig.update_layout(
                            title="Cross-Validation Scores Comparison",
                            xaxis_title="Model",
                            yaxis_title="CV Score",
                            height=400,
                            showlegend=False,
                            yaxis_range=[0, 1.05]
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        st.markdown("### üìä Model Ranking")
                        
                        # Sort by CV score
                        sorted_models = sorted(all_models_data, key=lambda x: x['CV Score (numeric)'], reverse=True)
                        
                        for i, model in enumerate(sorted_models, 1):
                            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
                            st.markdown(f"{medal} **{model['Model']}**")
                            st.caption(f"CV Score: {model['CV Score']}")
                
                # Show detailed parameters in expander
                with st.expander("üîß View All Model Parameters"):
                    for iter_key, iter_data in results.get('all_iterations', {}).items():
                        if 'model_tuning' in iter_data:
                            model_tuning = iter_data['model_tuning']
                            for model in model_tuning.get('all_models', []):
                                is_best = model['algorithm'] == final_results['best_model']
                                status = "‚úÖ **SELECTED**" if is_best else ""
                                
                                st.markdown(f"**{model['algorithm'].replace('_', ' ').title()}** {status}")
                                st.write(f"CV Score: `{model['cv_score']:.6f}`")
                                st.json(model.get('best_params', {}))
                                st.divider()
            else:
                st.info("Model comparison data not available")
            
            # Metrics
            st.subheader("üìà Best Model Performance Metrics")
            
            metrics = final_results['metrics']
            task_type = st.session_state['task_type']
            
            # Plot metrics
            fig = plot_metrics(metrics, task_type)
            st.plotly_chart(fig, use_container_width=True)
            
            # Confusion matrix for classification
            if task_type == "classification":
                cm_fig = display_confusion_matrix(metrics)
                if cm_fig:
                    st.plotly_chart(cm_fig, use_container_width=True)
            
            # Detailed metrics
            with st.expander("üìã Detailed Metrics"):
                st.json(metrics)
            
            # Explainability Section
            st.subheader("üîç Model Explainability")
            
            # Get explainability data from evaluator results
            explainability = final_results.get('explainability', {})
            
            if explainability and explainability.get('methods_available'):
                # Summary table
                summary_df = create_explainability_summary_table(explainability)
                if not summary_df.empty:
                    st.dataframe(summary_df, use_container_width=True, hide_index=True)
                
                # Tabs for different explainability methods
                exp_tabs = st.tabs(["üìä Feature Importance", "üéØ SHAP Analysis", "üî¨ LIME Analysis", "‚öñÔ∏è Method Comparison"])
                
                with exp_tabs[0]:
                    st.markdown("### Built-in Feature Importance")
                    feature_imp = explainability.get('feature_importance', {})
                    if feature_imp:
                        # Create bar chart
                        features = list(feature_imp.keys())
                        values = list(feature_imp.values())
                        
                        fig = go.Figure(data=[
                            go.Bar(
                                x=values,
                                y=features,
                                orientation='h',
                                marker=dict(color='skyblue'),
                                text=[f'{v:.4f}' for v in values],
                                textposition='auto',
                            )
                        ])
                        
                        fig.update_layout(
                            title="Top 10 Most Important Features",
                            xaxis_title="Importance Score",
                            yaxis_title="Features",
                            height=max(400, len(features) * 40),
                            yaxis={'categoryorder': 'total ascending'}
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Feature importance not available for this model type")
                
                with exp_tabs[1]:
                    st.markdown("### SHAP (SHapley Additive exPlanations)")
                    st.caption("SHAP values show the impact of each feature on the model's predictions")
                    
                    if 'shap' in explainability.get('methods_available', []):
                        shap_importance = explainability.get('shap_feature_importance', {})
                        shap_summary = explainability.get('shap_summary', {})
                        
                        if shap_importance:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Samples Analyzed", shap_summary.get('samples_analyzed', 'N/A'))
                            with col2:
                                st.metric("Top Feature", shap_summary.get('top_feature', 'N/A'))
                            with col3:
                                st.metric("Status", "‚úÖ Complete" if shap_summary.get('analysis_complete') else "‚ö†Ô∏è Partial")
                            
                            # SHAP importance plot
                            fig_shap = plot_shap_feature_importance(shap_importance)
                            if fig_shap:
                                st.plotly_chart(fig_shap, use_container_width=True)
                            
                            with st.expander("‚ÑπÔ∏è Understanding SHAP Values"):
                                st.markdown("""
                                **SHAP (SHapley Additive exPlanations)** values:
                                - Based on game theory (Shapley values)
                                - Show each feature's contribution to the prediction
                                - Higher values = more important for the model
                                - Consistent and theoretically sound
                                - Works for any machine learning model
                                """)
                        else:
                            st.info("SHAP analysis not yet computed")
                    else:
                        shap_msg = explainability.get('shap_values', 'SHAP not enabled')
                        if 'not available' in str(shap_msg).lower() or 'not installed' in str(shap_msg).lower():
                            st.warning("üì¶ SHAP library not installed. Install with: `pip install shap`")
                        else:
                            st.info("SHAP analysis not enabled in configuration")
                
                with exp_tabs[2]:
                    st.markdown("### LIME (Local Interpretable Model-agnostic Explanations)")
                    st.caption("LIME explains individual predictions by approximating the model locally")
                    
                    if 'lime' in explainability.get('methods_available', []):
                        lime_importance = explainability.get('lime_feature_importance', {})
                        lime_summary = explainability.get('lime_summary', {})
                        lime_sample_explanations = explainability.get('lime_sample_explanations', [])
                        
                        if lime_importance:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Instances Explained", lime_summary.get('samples_explained', 'N/A'))
                            with col2:
                                st.metric("Top Feature", lime_summary.get('top_feature', 'N/A'))
                            with col3:
                                st.metric("Status", "‚úÖ Complete" if lime_summary.get('analysis_complete') else "‚ö†Ô∏è Partial")
                            
                            # LIME aggregate importance plot
                            fig_lime = plot_lime_feature_importance(lime_importance)
                            if fig_lime:
                                st.plotly_chart(fig_lime, use_container_width=True)
                            
                            # Individual instance explanations
                            if lime_sample_explanations:
                                st.markdown("#### Sample Instance Explanations")
                                for exp in lime_sample_explanations:
                                    with st.expander(f"Instance {exp['instance_index']}"):
                                        fig_instance = plot_lime_instance_explanation(exp, exp['instance_index'])
                                        if fig_instance:
                                            st.plotly_chart(fig_instance, use_container_width=True)
                            
                            with st.expander("‚ÑπÔ∏è Understanding LIME"):
                                st.markdown("""
                                **LIME (Local Interpretable Model-agnostic Explanations)**:
                                - Explains individual predictions
                                - Creates a simple, interpretable model locally around the prediction
                                - Shows which features pushed the prediction in positive/negative direction
                                - Model-agnostic (works with any model)
                                - Great for understanding specific cases
                                """)
                        else:
                            st.info("LIME analysis not yet computed")
                    else:
                        lime_msg = explainability.get('lime_explanations', 'LIME not enabled')
                        if 'not available' in str(lime_msg).lower() or 'not installed' in str(lime_msg).lower():
                            st.warning("üì¶ LIME library not installed. Install with: `pip install lime`")
                        else:
                            st.info("LIME analysis not enabled in configuration")
                
                with exp_tabs[3]:
                    st.markdown("### Comparing Explainability Methods")
                    st.caption("Different methods may rank features differently based on their approach")
                    
                    # Comparison chart
                    fig_compare = compare_explainability_methods(explainability)
                    if fig_compare:
                        st.plotly_chart(fig_compare, use_container_width=True)
                        
                        st.markdown("#### Key Differences:")
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.markdown("**Feature Importance**")
                            st.caption("‚Ä¢ Model's built-in measure")
                            st.caption("‚Ä¢ Global interpretation")
                            st.caption("‚Ä¢ Fast to compute")
                        
                        with col2:
                            st.markdown("**SHAP**")
                            st.caption("‚Ä¢ Game theory based")
                            st.caption("‚Ä¢ Global & local view")
                            st.caption("‚Ä¢ Theoretically sound")
                        
                        with col3:
                            st.markdown("**LIME**")
                            st.caption("‚Ä¢ Local approximation")
                            st.caption("‚Ä¢ Instance-specific")
                            st.caption("‚Ä¢ Model-agnostic")
                    else:
                        st.info("Need at least 2 explainability methods to compare")
            else:
                st.info("üîß Explainability analysis not available. Enable in config.yaml:\n```yaml\nevaluator:\n  explainability_methods: [\"shap\", \"lime\"]\n```")
            
            # Recommendations
            st.subheader("üí° Recommendations")
            for i, rec in enumerate(results['recommendations'], 1):
                st.write(f"{i}. {rec}")
            
            # Download results
            st.subheader("üíæ Download Results")
            
            col1, col2 = st.columns(2)
            
            with col1:
                results_json = json.dumps(results, indent=2, default=str)
                st.download_button(
                    "üì• Download Results (JSON)",
                    data=results_json,
                    file_name=f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
            
            with col2:
                # Save model button
                if st.button("üíæ Save Model", use_container_width=True):
                    try:
                        orchestrator = st.session_state['orchestrator']
                        model_path = f"models/model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                        orchestrator.save_final_model(model_path)
                        st.success(f"‚úÖ Model saved to {model_path}")
                    except Exception as e:
                        st.error(f"Error saving model: {e}")
        
        else:
            st.info("üëÜ Upload a dataset and run the pipeline to see results here.")
    
    with tab3:
        st.header("ü§ñ Agent Communication")
        
        if 'orchestrator' in st.session_state:
            orchestrator = st.session_state['orchestrator']
            messages = orchestrator.get_message_history()
            
            st.metric("Total Messages", len(messages))
            
            # Message timeline
            for i, msg in enumerate(messages, 1):
                with st.container():
                    col1, col2 = st.columns([1, 4])
                    
                    with col1:
                        st.caption(f"Message {i}")
                        st.caption(msg['timestamp'][:19])
                    
                    with col2:
                        msg_type = msg['message_type']
                        
                        # Color schemes for different agents and message types
                        agent_colors = {
                            'orchestrator': '#9c27b0',  # Purple
                            'eda': '#2196f3',            # Blue
                            'feature_engineering': '#009688',  # Teal
                            'model_tuning': '#ff9800',   # Orange
                            'evaluator': '#4caf50',      # Green
                            'judge': '#f44336'           # Red
                        }
                        
                        msg_type_colors = {
                            'request': '#2196f3',
                            'response': '#4caf50',
                            'decision': '#ff9800',
                            'error': '#f44336'
                        }
                        
                        sender_color = agent_colors.get(msg["sender"].lower(), '#757575')
                        receiver_color = agent_colors.get(msg["receiver"].lower(), '#757575')
                        type_color = msg_type_colors.get(msg_type, '#757575')
                        
                        st.markdown(
                            f'<div style="padding: 12px; border-radius: 8px; '
                            f'background: linear-gradient(135deg, {sender_color}15 0%, {receiver_color}15 100%); '
                            f'border-left: 4px solid {sender_color};">'
                            f'<span style="background: {sender_color}; color: white; padding: 4px 12px; '
                            f'border-radius: 12px; font-weight: bold; font-size: 14px;">'
                            f'{msg["sender"]}</span> '
                            f'<span style="color: #666; margin: 0 8px;">‚Üí</span> '
                            f'<span style="background: {receiver_color}; color: white; padding: 4px 12px; '
                            f'border-radius: 12px; font-weight: bold; font-size: 14px;">'
                            f'{msg["receiver"]}</span> '
                            f'<span style="float: right; background: {type_color}; color: white; '
                            f'padding: 4px 12px; border-radius: 12px; font-size: 12px; font-weight: bold;">'
                            f'{msg_type.upper()}</span>'
                            f'</div>',
                            unsafe_allow_html=True
                        )
                    
                    st.divider()
        else:
            st.info("üëÜ Run the pipeline to see agent communication here.")
    
    with tab4:
        st.header("üìà Visualizations")
        
        if 'results' in st.session_state:
            results = st.session_state['results']
            
            # Performance over iterations
            iterations = results.get('all_iterations', {})
            
            if len(iterations) > 1:
                st.subheader("üìä Performance Across Iterations")
                
                iter_nums = []
                scores = []
                
                for key, value in iterations.items():
                    if 'judgment' in value:
                        iter_num = int(key.split('_')[1])
                        score = value['judgment']['judgment']['performance_score']
                        iter_nums.append(iter_num)
                        scores.append(score)
                
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=iter_nums,
                    y=scores,
                    mode='lines+markers',
                    name='Performance Score',
                    line=dict(color='#1f77b4', width=3),
                    marker=dict(size=10)
                ))
                
                fig.update_layout(
                    title="Performance Score Across Iterations",
                    xaxis_title="Iteration",
                    yaxis_title="Score",
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Pipeline execution time
            st.subheader("‚è±Ô∏è Execution Time")
            
            pipeline_info = results['pipeline_info']
            exec_time = pipeline_info['execution_time']
            
            st.metric("Total Execution Time", f"{exec_time:.2f} seconds")
            
            # Time breakdown (placeholder - would need actual timing data)
            stages = ['EDA', 'Feature Eng.', 'Model Tuning', 'Evaluation', 'Judge']
            times = [exec_time * 0.1, exec_time * 0.15, exec_time * 0.5, exec_time * 0.15, exec_time * 0.1]
            
            fig = px.pie(
                values=times,
                names=stages,
                title="Time Distribution by Stage (Estimated)"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        else:
            st.info("üëÜ Run the pipeline to see visualizations here.")
    
    with tab5:
        st.header("üîÑ Compare Multiple Runs")
        
        all_runs = run_history.get_all_runs()
        
        if len(all_runs) >= 2:
            # Multi-select for runs to compare
            run_options = {
                f"{run['run_name']} - {run['model_name']} ({run['timestamp'][:19]})": run['run_id']
                for run in all_runs
            }
            
            selected_runs = st.multiselect(
                "Select runs to compare (2-5 runs)",
                options=list(run_options.keys()),
                max_selections=5
            )
            
            if len(selected_runs) >= 2:
                selected_ids = [run_options[run] for run in selected_runs]
                
                # Get comparison data
                comparison_df = run_history.get_comparison_data(selected_ids)
                
                st.subheader("üìä Side-by-Side Comparison")
                st.dataframe(comparison_df, use_container_width=True)
                
                # Visualize metrics comparison
                st.subheader("üìà Metrics Comparison")
                
                # Determine which metrics to plot based on task type
                first_run = run_history.get_run_metadata(selected_ids[0])
                task_type = first_run['task_type']
                
                if task_type == "classification":
                    metric_cols = ['accuracy', 'f1_score', 'precision', 'recall']
                    metric_labels = ['Accuracy', 'F1 Score', 'Precision', 'Recall']
                else:
                    metric_cols = ['r2_score', 'rmse', 'mae']
                    metric_labels = ['R¬≤ Score', 'RMSE', 'MAE']
                
                # Create grouped bar chart
                fig = go.Figure()
                
                for i, metric in enumerate(metric_cols):
                    if metric in comparison_df.columns:
                        fig.add_trace(go.Bar(
                            name=metric_labels[i],
                            x=comparison_df['Run Name'],
                            y=comparison_df[metric],
                            text=comparison_df[metric].round(3),
                            textposition='auto',
                        ))
                
                fig.update_layout(
                    title=f"{task_type.capitalize()} Metrics Comparison",
                    xaxis_title="Run",
                    yaxis_title="Score",
                    barmode='group',
                    height=500,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Best performer
                st.subheader("üèÜ Best Performer")
                
                if task_type == "classification":
                    best_idx = comparison_df['accuracy'].idxmax()
                    best_metric = 'Accuracy'
                    best_value = comparison_df.loc[best_idx, 'accuracy']
                else:
                    best_idx = comparison_df['r2_score'].idxmax()
                    best_metric = 'R¬≤ Score'
                    best_value = comparison_df.loc[best_idx, 'r2_score']
                
                best_run = comparison_df.loc[best_idx, 'Run Name']
                best_model = comparison_df.loc[best_idx, 'Model']
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ü•á Best Run", best_run)
                with col2:
                    st.metric("üéØ Best Model", best_model)
                with col3:
                    st.metric(f"üìä Best {best_metric}", f"{best_value:.3f}")
                
                # Export comparison
                st.subheader("üíæ Export Comparison")
                col1, col2 = st.columns(2)
                
                with col1:
                    csv = comparison_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download as CSV",
                        data=csv,
                        file_name=f"run_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                with col2:
                    json_data = comparison_df.to_json(orient='records', indent=2)
                    st.download_button(
                        label="üì• Download as JSON",
                        data=json_data,
                        file_name=f"run_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                        use_container_width=True
                    )
            
            elif len(selected_runs) == 1:
                st.info("Please select at least 2 runs to compare.")
            else:
                st.info("Select runs from the dropdown above to start comparing.")
        
        elif len(all_runs) == 1:
            st.warning("You need at least 2 runs to compare. Run the pipeline again to create more runs!")
        else:
            st.info("No runs available yet. Start by running your first pipeline!")
    
    # Footer
    st.divider()
    st.caption("ü§ñ Explainable ML Pipeline | Built with Streamlit & LangChain | Powered by Llama 3.1")


if __name__ == "__main__":
    main()

